{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import libraries","metadata":{"id":"JzEnT6CS7rUp"}},{"cell_type":"code","source":"import re\nimport pickle\nimport sklearn\nimport pandas as pd\nimport numpy as np\nimport holoviews as hv\nimport nltk \nimport string\nimport tensorflow as tf\n\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\nfrom numpy import array\nfrom pickle import dump\nfrom keras.layers import Dense\nfrom keras.layers import LSTM, GRU, Dropout\nfrom keras.layers import Embedding\nfrom keras.models import Sequential\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.preprocessing.text import Tokenizer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","id":"QRSzGFa17rUw","execution":{"iopub.status.busy":"2022-04-08T01:33:10.558169Z","iopub.execute_input":"2022-04-08T01:33:10.558866Z","iopub.status.idle":"2022-04-08T01:33:10.565271Z","shell.execute_reply.started":"2022-04-08T01:33:10.558829Z","shell.execute_reply":"2022-04-08T01:33:10.564554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read data","metadata":{"id":"57Fdzmxy7rUz"}},{"cell_type":"code","source":"'''\n1- Open the file\n2- Read data(the data is cleaned)\n3- Close the file\n'''\n# Open file\nfile = open('../input/dataset-clean-1/republic_clean.txt', 'r')\n# Text file content (all data)\ntext = file.read()      \n# Close file\nfile.close()  ","metadata":{"id":"69GchmyL7rU2","execution":{"iopub.status.busy":"2022-04-08T01:33:11.361647Z","iopub.execute_input":"2022-04-08T01:33:11.362526Z","iopub.status.idle":"2022-04-08T01:33:11.385819Z","shell.execute_reply.started":"2022-04-08T01:33:11.362472Z","shell.execute_reply":"2022-04-08T01:33:11.385111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Print the first 500 characters from the text file\nprint(text[:500])","metadata":{"id":"y7MgNH3S7rU3","outputId":"e7b3c4e8-7a5f-48cb-fa05-15b85bc4a09e","execution":{"iopub.status.busy":"2022-04-08T01:33:11.734862Z","iopub.execute_input":"2022-04-08T01:33:11.735150Z","iopub.status.idle":"2022-04-08T01:33:11.739840Z","shell.execute_reply.started":"2022-04-08T01:33:11.735118Z","shell.execute_reply":"2022-04-08T01:33:11.738868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cleaning data","metadata":{"id":"N0m6gmS47rU5"}},{"cell_type":"markdown","source":"We want to change all text to words or tokens to use it to train models.\nSo we will clean the text to be readu to use it.\n- Steps:\n\n>1) Remove white space.\n\n>2) Keep only ASCII, no digits.\n\n>3) remove single letter chars.\n\n>4) Replace -- with white space.\n\n>5) Split all data into tokens by white space.\n\n>6) Remove all non-alphabetic tokens.\n\n>7) Convert all tokens into lowercase.\n\n>8) remove punctuation. \n\nWe will implement each of these cleaning operations  in a function. Below is the function clean_text() that takes a file of text as an argument and returns an array of clean tokens.\n","metadata":{}},{"cell_type":"code","source":"def clean_text(text):\n    \"\"\" steps:\n        - Remove all white spaces.\n        - Keep only ASCII, no digits\n        - remove single letter chars\n        - Replace -- with white space.\n        - Split all data into tokens by white space.\n        - Remove all non-alphabetic tokens.\n        - Convert all tokens into lowercase.\n        - remove punctuation \n    \"\"\"  \n    \n    RE_WSPACE = re.compile(r\"\\s+\", re.IGNORECASE)  #White Space\n    RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž ]\", re.IGNORECASE)    #ASCII\n    RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž]\\b\", re.IGNORECASE) #Single charachter between two spaces\n    RE_TAGS = re.compile(r\"<[^>]+>\")    #Tags\n    \n    #Replace '--' with a space ' '\n    text = text.replace('--', ' ')\n    #Remove tags\n    text = re.sub(RE_TAGS, \" \", text)\n    #Remove any non english character with a single space.\n    text = re.sub(RE_ASCII, \" \", text)\n    #Remove single charachter between two spaces \n    text = re.sub(RE_SINGLECHAR, \" \", text)\n    #Replace White Space, Tags, ASCII and Single charachter between two spaces with single space\n    text = re.sub(RE_WSPACE, \" \", text)\n    # split into tokens by white space\n    tokens = text.split()\n    # remove punctuation from each token\n    tokens = [t.translate(str.maketrans('', '', string.punctuation)) for t in tokens]\n    # make lower case\n    tokens = [word.lower() for word in tokens]\n    return tokens","metadata":{"id":"SCDZBNRl7rU6","execution":{"iopub.status.busy":"2022-04-08T01:33:12.892965Z","iopub.execute_input":"2022-04-08T01:33:12.893605Z","iopub.status.idle":"2022-04-08T01:33:12.901659Z","shell.execute_reply.started":"2022-04-08T01:33:12.893560Z","shell.execute_reply":"2022-04-08T01:33:12.900976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# clean text\n# print out some of the tokens\ntokens = clean_text(text)\nprint(tokens[:200])\nprint('Total Tokens: %d' % len(tokens))\nprint('Unique Tokens: %d' % len(set(tokens)))","metadata":{"id":"18ZejrA97rU-","outputId":"aa4748d5-de31-49ae-8642-9081b16cac29","execution":{"iopub.status.busy":"2022-04-08T01:33:13.223803Z","iopub.execute_input":"2022-04-08T01:33:13.224411Z","iopub.status.idle":"2022-04-08T01:33:13.902534Z","shell.execute_reply.started":"2022-04-08T01:33:13.224370Z","shell.execute_reply":"2022-04-08T01:33:13.901644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Splitting the data into sequences","metadata":{"id":"quFyJN0V7rU_"}},{"cell_type":"markdown","source":"* We will split tokens into sequences of 50 input words and 1 output word.\n* Each line has 50 input + 1 output = 51 word.\n* Printing statistics on the list, we can see that Total Sequences: 211391 training patterns to fit our model.","metadata":{}},{"cell_type":"code","source":"'''\n1- We will split tokens into sequences of 50 input words and 1 output word.\n2- Each line has 50 input + 1 output = 51 word.\n'''\n\n# organize into sequences of tokens\nsequences = list()\nfor i in range(51, len(tokens)):\n    # select sequence of tokens\n    seq = tokens[i-51:i]\n    # convert into a line\n    line = ' '.join(seq)\n    # save the line \n    sequences.append(line)\nprint('Total Sequences: %d' % len(sequences))","metadata":{"id":"bsni3wTT7rVB","outputId":"f3b254fd-047a-4800-c978-b76e04690d74","execution":{"iopub.status.busy":"2022-04-08T01:33:14.407669Z","iopub.execute_input":"2022-04-08T01:33:14.408349Z","iopub.status.idle":"2022-04-08T01:33:14.863571Z","shell.execute_reply.started":"2022-04-08T01:33:14.408298Z","shell.execute_reply":"2022-04-08T01:33:14.862764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Save the sequence of lines in the file to use this file.","metadata":{"id":"wVX3q3zH7rVC"}},{"cell_type":"code","source":"'''\n1- Add all lines with \\n in the data variable.\n2- Open file to write and save sequence data.\n3- Write the data in the file.\n'''\ndata = '\\n'.join(sequences)\nfile = open('sequence_line_file.txt', 'w')\nfile.write(data)\nfile.close()","metadata":{"id":"qszsYKIo7rVD","execution":{"iopub.status.busy":"2022-04-08T01:33:15.131662Z","iopub.execute_input":"2022-04-08T01:33:15.132149Z","iopub.status.idle":"2022-04-08T01:33:15.293884Z","shell.execute_reply.started":"2022-04-08T01:33:15.132113Z","shell.execute_reply":"2022-04-08T01:33:15.292977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read Train text after cleaning and splitting","metadata":{"id":"zeJITVwO7rVE"}},{"cell_type":"code","source":"'''\n1- Open the file\n2- Read data(the data is cleaned)\n3- Close the file\n'''\nfile = open('sequence_line_file.txt', 'r')\n# Read text file content\ntext = file.read()  \n# Close file\nfile.close()","metadata":{"id":"_htUDxII7rVE","execution":{"iopub.status.busy":"2022-04-08T01:33:15.869627Z","iopub.execute_input":"2022-04-08T01:33:15.870209Z","iopub.status.idle":"2022-04-08T01:33:15.949069Z","shell.execute_reply.started":"2022-04-08T01:33:15.870167Z","shell.execute_reply":"2022-04-08T01:33:15.948232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Splitting text based on new lines.\nlines = text.split('\\n')\nlines","metadata":{"id":"OCEUBEoR7rVF","outputId":"614ba015-0cd8-43f5-fc3f-5145e18d8370","execution":{"iopub.status.busy":"2022-04-08T01:33:16.572753Z","iopub.execute_input":"2022-04-08T01:33:16.573378Z","iopub.status.idle":"2022-04-08T01:33:16.687582Z","shell.execute_reply.started":"2022-04-08T01:33:16.573338Z","shell.execute_reply":"2022-04-08T01:33:16.686826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train a statistical language model","metadata":{"id":"1KHJfVgS7rVG"}},{"cell_type":"markdown","source":"In this section, we will need to prepare data or tokens to apply the embedding layers in the data.\n\n**Embedding layer** steps: after tokenizing the sentences into words.\n\n    1- Convert the text or tokenizer into integer numbers.\n    2- Splitting the tokenizer into X and y.\n    3- Create a one-hot encoded vector for each y.\n    4- Pass X and, y as an input of the embedded layer.","metadata":{"id":"CgtiOiDiFADO"}},{"cell_type":"markdown","source":"**To implement the word embedding layer we should convert input sequences into integers.**\n\n**Tokenizer converts all unique words into unique integer numbers, and then we will convert all input text into numbers by using these unique numbers.**","metadata":{"id":"bP1R7ztI7rVH"}},{"cell_type":"code","source":"# integer encode sequences of words\ntoken = Tokenizer()\ntoken.fit_on_texts(lines)\nsequences = token.texts_to_sequences(lines)\nsequences","metadata":{"id":"eGWKmCRO7rVI","outputId":"c16b588a-f646-454d-bb3d-853dbddb23da","execution":{"iopub.status.busy":"2022-04-08T01:33:19.043653Z","iopub.execute_input":"2022-04-08T01:33:19.043928Z","iopub.status.idle":"2022-04-08T01:33:35.051768Z","shell.execute_reply.started":"2022-04-08T01:33:19.043897Z","shell.execute_reply":"2022-04-08T01:33:35.050855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"''' \n    We need to define the embedding layer so we need the size of the vocabulary.\n    So we used word_index to list mapping words to their rank/index (int) and \n    set it after fit_text_tokenizer() is called on the tokenizer.\n'''\n# vocabulary size\nvocab_len = len(token.word_index) + 1\nvocab_len","metadata":{"id":"tOFIhwMS7rVK","outputId":"e88e31ad-c53f-4010-ea24-79266a74969d","execution":{"iopub.status.busy":"2022-04-08T01:33:35.054255Z","iopub.execute_input":"2022-04-08T01:33:35.054783Z","iopub.status.idle":"2022-04-08T01:33:35.061824Z","shell.execute_reply.started":"2022-04-08T01:33:35.054738Z","shell.execute_reply":"2022-04-08T01:33:35.061122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Splitting the data into inputs(X) and output(y)","metadata":{"execution":{"iopub.status.busy":"2022-04-07T20:45:33.356288Z","iopub.execute_input":"2022-04-07T20:45:33.357103Z","iopub.status.idle":"2022-04-07T20:45:33.365747Z","shell.execute_reply.started":"2022-04-07T20:45:33.35706Z","shell.execute_reply":"2022-04-07T20:45:33.364402Z"},"id":"jrA1pcmA7rVL"}},{"cell_type":"code","source":"#Change sequences from list to array to we can split data into X and y easly\nsequences = array(sequences)\nsequences","metadata":{"id":"5QxMcRL87rVL","outputId":"93a8ba9c-2eb1-494f-b004-2f51abfada39","execution":{"iopub.status.busy":"2022-04-08T01:33:35.063122Z","iopub.execute_input":"2022-04-08T01:33:35.063622Z","iopub.status.idle":"2022-04-08T01:33:36.434203Z","shell.execute_reply.started":"2022-04-08T01:33:35.063583Z","shell.execute_reply":"2022-04-08T01:33:36.433135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Split sequences into X and y\nX, y = sequences[:,:-1], sequences[:,-1]\n#One hot encoder(y)\ny = to_categorical(y, num_classes=vocab_len)","metadata":{"id":"F55ax7UH7rVL","execution":{"iopub.status.busy":"2022-04-08T01:33:36.436421Z","iopub.execute_input":"2022-04-08T01:33:36.436942Z","iopub.status.idle":"2022-04-08T01:33:36.871278Z","shell.execute_reply.started":"2022-04-08T01:33:36.436897Z","shell.execute_reply":"2022-04-08T01:33:36.870433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{"id":"4Y33G3rZ7rVM"}},{"cell_type":"markdown","source":"In this part we will built our models using STML once, and once again, we will use GRU.","metadata":{}},{"cell_type":"markdown","source":"## LSTM","metadata":{"id":"10ag3yOz7rVN"}},{"cell_type":"markdown","source":"In this part we will built our models using STML once, and once again, we will use GRU\\\nthe following steps that we used it:\n1. Added embedding layer, it is very important to determine the vocabulary size and input sequences length. It takes\n    1. input_dim = length of vocabulary \n    2. output_dim = Dimension of the dense embedding\n    3. input_length = Length of input sequences\n2. Added LSTM(Long Short-Term) layer. It takes\n    1. units = 115 number of units that means dimensionality of the output space\n    2. return_sequences used to return the last output\n3. Added Dropout layer with rate equal to 20 % that it used to prevent overfitting\n4. Added Dense layer \n    1. 50 units that refer to the dimensionality of the output space\n    2. ReLU activation function\n5.  Added Dense layer \n    1. Vocabulary length as units that refer to the dimensionality of the output space\n    2. Softmax activation function\nthen print the summary of the model, compile, and fit the model with batch size equal to 100, epochs = 100, and early stopping to prevent over fitting","metadata":{}},{"cell_type":"code","source":"model = Sequential()\n# Embedding layer used to convert each word into a fixed length vector\nmodel.add(Embedding(vocab_len, 50, input_length=X.shape[1]))\n# LSTM(Long Short-Term) is actually a kind of RNN architecture\nmodel.add(LSTM(115, return_sequences=True)) \nmodel.add(LSTM(115))\nmodel.add(Dropout(0.2))# 20% dropout\n# Dense layer is the regular deeply connected neural network layer.\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dense(vocab_len, activation='softmax'))\n# Display the structure of the model\nprint(model.summary())","metadata":{"id":"COQvNjiJ7rVN","execution":{"iopub.status.busy":"2022-04-08T00:23:12.472212Z","iopub.execute_input":"2022-04-08T00:23:12.472700Z","iopub.status.idle":"2022-04-08T00:23:15.388378Z","shell.execute_reply.started":"2022-04-08T00:23:12.472661Z","shell.execute_reply":"2022-04-08T00:23:15.387593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training the model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n# fit model\nmodel.fit(X, y, batch_size=50, epochs=70, callbacks=[\n        tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=5)\n    ])","metadata":{"id":"tlWAgMD37rVN","execution":{"iopub.status.busy":"2022-04-08T00:23:15.389873Z","iopub.execute_input":"2022-04-08T00:23:15.390157Z","iopub.status.idle":"2022-04-08T01:29:56.955596Z","shell.execute_reply.started":"2022-04-08T00:23:15.390122Z","shell.execute_reply":"2022-04-08T01:29:56.954831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this part we will built our models using LSTM model\\\nthe following steps that we used it:\n1. Added embedding layer, it is very important to determine the vocabulary size and input sequences length. It takes\n    1. input_dim = length of vocabulary \n    2. output_dim = Dimension of the dense embedding\n    3. input_length = Length of input sequences\n2. Added LSTM(Long Short-Term) layer. It takes\n    1. units = 115 number of units that means dimensionality of the output space\n    2. return_sequences used to return the last output\n3. Added LSTM(Long Short-Term) layer. It takes\n    1. units = 115 number of units that means dimensionality of the output space\n4. Added Dropout layer with rate equal to 20 % that it used to prevent overfitting\n5. Added Dense layer \n    1. 50 units that refer to the dimensionality of the output space\n    2. ReLU activation function\n6.  Added Dense layer \n    1. Vocabulary length as units that refer to the dimensionality of the output space\n    2. Softmax activation function\nthen print the summary of the model, compile (adam otimizer), and fit the model with batch size equal to 50, epochs = 70, and early stopping to prevent over fitting with 5 patience","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## GRU","metadata":{"id":"TKexlPjf7rVO"}},{"cell_type":"code","source":"model1 = Sequential()\n# Embedding layer used to convert each word into a fixed length vector\nmodel1.add(Embedding(vocab_len, 50, input_length=X.shape[1]))\n# GRU (Gated Recurrent Unit) is a variation on the recurrent neural network design and\n# It is similar to long-term short-term memory cells\nmodel1.add(GRU(112))\nmodel1.add(Dropout(0.2)) # 20% dropout\n# Dense layer is the regular deeply connected neural network layer.\nmodel1.add(Dense(50, activation='relu'))\nmodel1.add(Dense(vocab_len, activation='softmax'))\n# Display the structure of the model\nprint(model1.summary())","metadata":{"id":"ojgOfz-g7rVO","execution":{"iopub.status.busy":"2022-04-08T01:34:38.080606Z","iopub.execute_input":"2022-04-08T01:34:38.080917Z","iopub.status.idle":"2022-04-08T01:34:38.290471Z","shell.execute_reply.started":"2022-04-08T01:34:38.080884Z","shell.execute_reply":"2022-04-08T01:34:38.289721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training the model\nmodel1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n# fit model\nmodel1.fit(X, y, batch_size=100, epochs=100, callbacks=[\n        tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=5)\n    ])","metadata":{"id":"K5fDY5_V7rVP","execution":{"iopub.status.busy":"2022-04-08T01:34:52.521448Z","iopub.execute_input":"2022-04-08T01:34:52.522239Z","iopub.status.idle":"2022-04-08T02:11:52.867918Z","shell.execute_reply.started":"2022-04-08T01:34:52.522193Z","shell.execute_reply":"2022-04-08T02:11:52.867218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this part we will built our models using GRU model\\\nthe following steps that we used it:\n1. Added embedding layer, it is very important to determine the vocabulary size and input sequences length. It takes\n    1. input_dim = length of vocabulary \n    2. output_dim = Dimension of the dense embedding\n    3. input_length = Length of input sequences\n2. Added GRU(Gated Recurrent Unit) layer. It takes\n    1. units = 112 number of units that means dimensionality of the output space\n    2. return_sequences used to return the last output\n3. Added GRU(Gated Recurrent Unit) layer. It takes\n    1. units = 112 number of units that means dimensionality of the output space\n4. Added Dropout layer with rate equal to 20 % that it used to prevent overfitting\n5. Added Dense layer \n    1. 50 units that refer to the dimensionality of the output space\n    2. ReLU activation function\n6.  Added Dense layer \n    1. Vocabulary length as units that refer to the dimensionality of the output space\n    2. Softmax activation function\nthen print the summary of the model, compile(adam otimizer), and fit the model with batch size equal to 100, epochs = 150, and early stopping to prevent over fitting with 5 patience","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}